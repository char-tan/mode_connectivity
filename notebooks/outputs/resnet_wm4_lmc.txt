Files already downloaded and verified
Files already downloaded and verified

performing naive interpolation
Average loss: 0.0002, Accuracy: (100%)
Average loss: 0.2784, Accuracy: (94%)
point 1/20. lam = 0.0, train loss = 0.00021626579135656358, test loss = 0.2784376403808594
Average loss: 0.0006, Accuracy: (100%)
Average loss: 0.2708, Accuracy: (94%)
point 2/20. lam = 0.05263157933950424, train loss = 0.0005716440504789352, test loss = 0.2707646667480469
Average loss: 0.0024, Accuracy: (100%)
Average loss: 0.2820, Accuracy: (93%)
point 3/20. lam = 0.10526315867900848, train loss = 0.0024140050625801085, test loss = 0.2820044616699219
Average loss: 0.0148, Accuracy: (100%)
Average loss: 0.3240, Accuracy: (92%)
point 4/20. lam = 0.15789473056793213, train loss = 0.014828957920074463, test loss = 0.3240397277832031
Average loss: 0.0843, Accuracy: (97%)
Average loss: 0.4293, Accuracy: (88%)
point 5/20. lam = 0.21052631735801697, train loss = 0.0842717300415039, test loss = 0.42925225830078123
Average loss: 0.3933, Accuracy: (87%)
Average loss: 0.7056, Accuracy: (80%)
point 6/20. lam = 0.2631579041481018, train loss = 0.3933067205810547, test loss = 0.7055513916015625
Average loss: 1.4169, Accuracy: (59%)
Average loss: 1.4992, Accuracy: (59%)
point 7/20. lam = 0.31578946113586426, train loss = 1.4169433520507813, test loss = 1.49918740234375
Average loss: 3.1366, Accuracy: (25%)
Average loss: 3.0092, Accuracy: (28%)
point 8/20. lam = 0.3684210479259491, train loss = 3.1365720849609375, test loss = 3.00915859375
Average loss: 3.8774, Accuracy: (12%)
Average loss: 3.8358, Accuracy: (13%)
point 9/20. lam = 0.42105263471603394, train loss = 3.8773866259765626, test loss = 3.83584365234375
Average loss: 4.0401, Accuracy: (11%)
Average loss: 3.9883, Accuracy: (11%)
point 10/20. lam = 0.4736842215061188, train loss = 4.040084208984375, test loss = 3.988268115234375
Average loss: 3.0571, Accuracy: (18%)
Average loss: 3.0013, Accuracy: (21%)
point 11/20. lam = 0.5263158082962036, train loss = 3.0570879736328127, test loss = 3.001332763671875
Average loss: 1.3711, Accuracy: (55%)
Average loss: 1.4819, Accuracy: (54%)
point 12/20. lam = 0.5789473652839661, train loss = 1.371092568359375, test loss = 1.4818623046875
Average loss: 0.5051, Accuracy: (83%)
Average loss: 0.7244, Accuracy: (77%)
point 13/20. lam = 0.6315789222717285, train loss = 0.5050708111572265, test loss = 0.7243810302734375
Average loss: 0.1790, Accuracy: (94%)
Average loss: 0.4519, Accuracy: (86%)
point 14/20. lam = 0.6842105388641357, train loss = 0.17897009307861328, test loss = 0.45190803833007814
Average loss: 0.0646, Accuracy: (98%)
Average loss: 0.3528, Accuracy: (89%)
point 15/20. lam = 0.7368420958518982, train loss = 0.06455120536804199, test loss = 0.3528306457519531
Average loss: 0.0224, Accuracy: (99%)
Average loss: 0.3150, Accuracy: (91%)
point 16/20. lam = 0.7894736528396606, train loss = 0.022397365188598632, test loss = 0.3149643493652344
Average loss: 0.0083, Accuracy: (100%)
Average loss: 0.3027, Accuracy: (92%)
point 17/20. lam = 0.8421052694320679, train loss = 0.008264360008239746, test loss = 0.30272333984375
Average loss: 0.0037, Accuracy: (100%)
Average loss: 0.3050, Accuracy: (92%)
point 18/20. lam = 0.8947368264198303, train loss = 0.0036504421281814575, test loss = 0.3050243041992188
Average loss: 0.0018, Accuracy: (100%)
Average loss: 0.3167, Accuracy: (93%)
point 19/20. lam = 0.9473684430122375, train loss = 0.0018234049224853516, test loss = 0.31665831909179687
Average loss: 0.0012, Accuracy: (100%)
Average loss: 0.3349, Accuracy: (93%)
point 20/20. lam = 1.0, train loss = 0.001248979722261429, test loss = 0.33486650390625

permuting model
iteration 0 P_BG_1: progress 30.338359832763672
iteration 0 P_BG_0: progress 59.83211898803711
iteration 0 P_BG_0_IN_0: progress 22.820903778076172
iteration 0 P_BG_1_IN_0: progress 18.292757034301758
iteration 0 P_BG_2_IN_2: progress 14.767999649047852
iteration 0 P_BG_2: progress 102.52143859863281
iteration 0 P_BG_2_IN_0: progress 25.703208923339844
iteration 0 P_BG_1_IN_1: progress 7.961923599243164
iteration 0 P_BG_2_IN_1: progress 33.233482360839844
iteration 0 P_BG_0_IN_1: progress 5.827723026275635
iteration 0 P_BG_0_IN_2: progress 4.021452903747559
iteration 0 P_BG_1_IN_2: progress 7.541786193847656
iteration 1 P_BG_2: progress 2.49334716796875
iteration 1 P_BG_0: progress 1.827911376953125
iteration 1 P_BG_1_IN_2: progress 0.0
iteration 1 P_BG_2_IN_2: progress 15.526357650756836
iteration 1 P_BG_2_IN_1: progress 1.9223556518554688
iteration 1 P_BG_0_IN_1: progress 0.1530294418334961
iteration 1 P_BG_1: progress 9.362808227539062
iteration 1 P_BG_1_IN_0: progress 3.0522193908691406
iteration 1 P_BG_2_IN_0: progress 8.720405578613281
iteration 1 P_BG_1_IN_1: progress 3.0684432983398438
iteration 1 P_BG_0_IN_0: progress 0.19573974609375
iteration 1 P_BG_0_IN_2: progress 0.3106198310852051
iteration 2 P_BG_2: progress 2.780548095703125
iteration 2 P_BG_1_IN_1: progress 0.0
iteration 2 P_BG_1_IN_0: progress 0.0
iteration 2 P_BG_0: progress 1.048187255859375
iteration 2 P_BG_1: progress 5.1888885498046875
iteration 2 P_BG_0_IN_2: progress 0.24345111846923828
iteration 2 P_BG_2_IN_2: progress 0.8131942749023438
iteration 2 P_BG_2_IN_0: progress 7.2804412841796875
iteration 2 P_BG_0_IN_0: progress 0.18544769287109375
iteration 2 P_BG_2_IN_1: progress 2.9914703369140625
iteration 2 P_BG_1_IN_2: progress 6.4826812744140625
iteration 2 P_BG_0_IN_1: progress 0.35781383514404297
iteration 3 P_BG_2_IN_2: progress 0.0
iteration 3 P_BG_1: progress 1.219970703125
iteration 3 P_BG_0_IN_0: progress 0.0
iteration 3 P_BG_0_IN_2: progress 0.0
iteration 3 P_BG_1_IN_2: progress 0.27607154846191406
iteration 3 P_BG_0_IN_1: progress 0.0
iteration 3 P_BG_1_IN_1: progress 3.990201950073242
iteration 3 P_BG_2_IN_1: progress 0.0
iteration 3 P_BG_0: progress 0.72735595703125
iteration 3 P_BG_1_IN_0: progress 3.7997093200683594
iteration 3 P_BG_2_IN_0: progress 1.31219482421875
iteration 3 P_BG_2: progress 3.652008056640625
iteration 4 P_BG_1_IN_2: progress 0.0
iteration 4 P_BG_1_IN_0: progress 0.0
iteration 4 P_BG_2: progress 0.0
iteration 4 P_BG_0_IN_0: progress 0.24368667602539062
iteration 4 P_BG_2_IN_0: progress 0.8746490478515625
iteration 4 P_BG_0: progress 0.00067138671875
iteration 4 P_BG_1: progress 1.8515472412109375
iteration 4 P_BG_2_IN_1: progress 1.8571624755859375
iteration 4 P_BG_0_IN_1: progress 0.5695648193359375
iteration 4 P_BG_0_IN_2: progress 0.26178503036499023
iteration 4 P_BG_1_IN_1: progress 0.5864734649658203
iteration 4 P_BG_2_IN_2: progress 0.381378173828125
iteration 5 P_BG_1_IN_0: progress 0.4807548522949219
iteration 5 P_BG_0_IN_2: progress 0.0
iteration 5 P_BG_1_IN_2: progress 0.4245491027832031
iteration 5 P_BG_2: progress 0.828826904296875
iteration 5 P_BG_0_IN_1: progress 0.0
iteration 5 P_BG_0: progress 0.0
iteration 5 P_BG_2_IN_1: progress 1.8009490966796875
iteration 5 P_BG_2_IN_0: progress 1.422088623046875
iteration 5 P_BG_1_IN_1: progress 0.0
iteration 5 P_BG_1: progress 0.0
iteration 5 P_BG_0_IN_0: progress 0.0
iteration 5 P_BG_2_IN_2: progress 0.7341842651367188
iteration 6 P_BG_2_IN_1: progress 0.0
iteration 6 P_BG_1_IN_1: progress 0.0
iteration 6 P_BG_0: progress 0.0
iteration 6 P_BG_0_IN_1: progress 0.0
iteration 6 P_BG_1_IN_0: progress 0.0
iteration 6 P_BG_1: progress 0.0
iteration 6 P_BG_2: progress 0.63323974609375
iteration 6 P_BG_1_IN_2: progress 0.0
iteration 6 P_BG_0_IN_0: progress 0.0
iteration 6 P_BG_2_IN_2: progress 0.2929267883300781
iteration 6 P_BG_2_IN_0: progress 0.2115478515625
iteration 6 P_BG_0_IN_2: progress 0.0
iteration 7 P_BG_2_IN_2: progress 0.0
iteration 7 P_BG_1_IN_2: progress 0.0
iteration 7 P_BG_0_IN_2: progress 0.0
iteration 7 P_BG_2_IN_1: progress 0.369964599609375
iteration 7 P_BG_0_IN_1: progress 0.0
iteration 7 P_BG_1: progress 0.0491790771484375
iteration 7 P_BG_1_IN_1: progress 0.04279518127441406
iteration 7 P_BG_2_IN_0: progress 0.05877685546875
iteration 7 P_BG_2: progress 0.178985595703125
iteration 7 P_BG_0: progress 0.0
iteration 7 P_BG_0_IN_0: progress 0.0
iteration 7 P_BG_1_IN_0: progress 0.0243988037109375
iteration 8 P_BG_1_IN_2: progress 0.044361114501953125
iteration 8 P_BG_2_IN_1: progress 0.7683639526367188
iteration 8 P_BG_1_IN_0: progress 0.0
iteration 8 P_BG_0_IN_1: progress 0.0
iteration 8 P_BG_0_IN_2: progress 0.0
iteration 8 P_BG_1: progress 0.0
iteration 8 P_BG_2: progress 0.0672607421875
iteration 8 P_BG_2_IN_0: progress 0.1583251953125
iteration 8 P_BG_2_IN_2: progress 0.12129974365234375
iteration 8 P_BG_0: progress 0.0235595703125
iteration 8 P_BG_0_IN_0: progress 0.002552032470703125
iteration 8 P_BG_1_IN_1: progress 0.0
iteration 9 P_BG_2_IN_2: progress 0.0
iteration 9 P_BG_2_IN_0: progress 0.0
iteration 9 P_BG_1_IN_0: progress 0.06873321533203125
iteration 9 P_BG_1_IN_2: progress 0.0
iteration 9 P_BG_1: progress 0.00830078125
iteration 9 P_BG_1_IN_1: progress 0.04673194885253906
iteration 9 P_BG_0: progress 0.0
iteration 9 P_BG_0_IN_2: progress 0.07704448699951172
iteration 9 P_BG_0_IN_1: progress 0.031787872314453125
iteration 9 P_BG_2_IN_1: progress 0.0501556396484375
iteration 9 P_BG_0_IN_0: progress 0.0
iteration 9 P_BG_2: progress 0.054412841796875
iteration 10 P_BG_2: progress 0.0
iteration 10 P_BG_0_IN_0: progress 0.0
iteration 10 P_BG_0_IN_2: progress 0.0
iteration 10 P_BG_0_IN_1: progress 0.0
iteration 10 P_BG_2_IN_1: progress 0.1421661376953125
iteration 10 P_BG_2_IN_0: progress 0.20659637451171875
iteration 10 P_BG_1_IN_2: progress 0.0394744873046875
iteration 10 P_BG_1_IN_1: progress 0.0
iteration 10 P_BG_0: progress 0.0
iteration 10 P_BG_2_IN_2: progress 0.03452301025390625
iteration 10 P_BG_1: progress 0.0
iteration 10 P_BG_1_IN_0: progress 0.0111541748046875
iteration 11 P_BG_1_IN_0: progress 0.0
iteration 11 P_BG_1_IN_1: progress 0.0
iteration 11 P_BG_2: progress 0.09393310546875
iteration 11 P_BG_2_IN_2: progress 0.0413055419921875
iteration 11 P_BG_0_IN_0: progress 0.0
iteration 11 P_BG_0_IN_2: progress 0.0
iteration 11 P_BG_2_IN_0: progress 0.05484771728515625
iteration 11 P_BG_1_IN_2: progress 0.0
iteration 11 P_BG_0: progress 0.0
iteration 11 P_BG_2_IN_1: progress 0.12885284423828125
iteration 11 P_BG_1: progress 0.0
iteration 11 P_BG_0_IN_1: progress 0.0
iteration 12 P_BG_0_IN_2: progress 0.0
iteration 12 P_BG_0_IN_0: progress 0.0
iteration 12 P_BG_1_IN_1: progress 0.0
iteration 12 P_BG_0_IN_1: progress 0.0
iteration 12 P_BG_1: progress 0.0
iteration 12 P_BG_2: progress 0.0
iteration 12 P_BG_2_IN_0: progress 0.0
iteration 12 P_BG_2_IN_2: progress 0.0
iteration 12 P_BG_1_IN_2: progress 0.0
iteration 12 P_BG_0: progress 0.0
iteration 12 P_BG_1_IN_0: progress 0.0
iteration 12 P_BG_2_IN_1: progress 0.0

performing permuted interpolation
Average loss: 0.0002, Accuracy: (100%)
Average loss: 0.2784, Accuracy: (94%)
point 1/20. lam = 0.0, train loss = 0.00020830868303775788, test loss = 0.2784376403808594
Average loss: 0.0003, Accuracy: (100%)
Average loss: 0.2846, Accuracy: (94%)
point 2/20. lam = 0.05263157933950424, train loss = 0.000303372101187706, test loss = 0.2846289794921875
Average loss: 0.0009, Accuracy: (100%)
Average loss: 0.3021, Accuracy: (93%)
point 3/20. lam = 0.10526315867900848, train loss = 0.0008640632712841033, test loss = 0.3020812072753906
Average loss: 0.0048, Accuracy: (100%)
Average loss: 0.3331, Accuracy: (93%)
point 4/20. lam = 0.15789473056793213, train loss = 0.004798541893959046, test loss = 0.3330613403320313
Average loss: 0.0221, Accuracy: (99%)
Average loss: 0.3873, Accuracy: (92%)
point 5/20. lam = 0.21052631735801697, train loss = 0.022052905921936036, test loss = 0.3872648193359375
Average loss: 0.0865, Accuracy: (97%)
Average loss: 0.4842, Accuracy: (89%)
point 6/20. lam = 0.2631579041481018, train loss = 0.08647029983520507, test loss = 0.484229541015625
Average loss: 0.2378, Accuracy: (93%)
Average loss: 0.6549, Accuracy: (86%)
point 7/20. lam = 0.31578946113586426, train loss = 0.2378065673828125, test loss = 0.65487099609375
Average loss: 0.4736, Accuracy: (88%)
Average loss: 0.8890, Accuracy: (81%)
point 8/20. lam = 0.3684210479259491, train loss = 0.473559814453125, test loss = 0.8889903686523437
Average loss: 0.7061, Accuracy: (82%)
Average loss: 1.0976, Accuracy: (77%)
point 9/20. lam = 0.42105263471603394, train loss = 0.7060747814941406, test loss = 1.0976288452148437
Average loss: 0.7421, Accuracy: (81%)
Average loss: 1.1381, Accuracy: (75%)
point 10/20. lam = 0.4736842215061188, train loss = 0.7421039880371094, test loss = 1.1380665893554687
Average loss: 0.5687, Accuracy: (85%)
Average loss: 0.9811, Accuracy: (78%)
point 11/20. lam = 0.5263158082962036, train loss = 0.5687363330078125, test loss = 0.9810597412109375
Average loss: 0.3411, Accuracy: (90%)
Average loss: 0.7581, Accuracy: (82%)
point 12/20. lam = 0.5789473652839661, train loss = 0.3411036187744141, test loss = 0.758050146484375
Average loss: 0.1761, Accuracy: (94%)
Average loss: 0.5758, Accuracy: (86%)
point 13/20. lam = 0.6315789222717285, train loss = 0.17607977172851563, test loss = 0.5758238342285156
Average loss: 0.0758, Accuracy: (97%)
Average loss: 0.4603, Accuracy: (89%)
point 14/20. lam = 0.6842105388641357, train loss = 0.07577961288452148, test loss = 0.4603102783203125
Average loss: 0.0293, Accuracy: (99%)
Average loss: 0.3900, Accuracy: (90%)
point 15/20. lam = 0.7368420958518982, train loss = 0.02928071002960205, test loss = 0.390049609375
Average loss: 0.0117, Accuracy: (100%)
Average loss: 0.3527, Accuracy: (92%)
point 16/20. lam = 0.7894736528396606, train loss = 0.011715142154693603, test loss = 0.3526741455078125
Average loss: 0.0048, Accuracy: (100%)
Average loss: 0.3340, Accuracy: (92%)
point 17/20. lam = 0.8421052694320679, train loss = 0.004789977426528931, test loss = 0.33399148559570313
Average loss: 0.0022, Accuracy: (100%)
Average loss: 0.3272, Accuracy: (93%)
point 18/20. lam = 0.8947368264198303, train loss = 0.002242225980758667, test loss = 0.32723317260742185
Average loss: 0.0015, Accuracy: (100%)
Average loss: 0.3280, Accuracy: (93%)
point 19/20. lam = 0.9473684430122375, train loss = 0.0015011181831359863, test loss = 0.32798870849609374
Average loss: 0.0013, Accuracy: (100%)
Average loss: 0.3349, Accuracy: (93%)
point 20/20. lam = 1.0, train loss = 0.0012846182250976562, test loss = 0.33486649780273436
[100.0, 99.992, 99.97, 99.598, 97.266, 87.462, 58.996, 24.552, 12.044, 10.652, 18.436, 55.286, 82.676, 94.24, 98.144, 99.482, 99.866, 99.942, 99.972, 99.972]
[94.05, 93.88, 93.17, 91.51, 88.23, 80.37, 59.0, 27.6, 12.59, 11.36, 21.01, 54.26, 76.68, 85.67, 89.05, 90.65, 91.65, 92.35, 92.6, 92.66]
[100.0, 100.0, 99.984, 99.858, 99.264, 97.28, 93.136, 87.506, 82.332, 81.12, 84.656, 89.77, 94.202, 97.326, 98.976, 99.66, 99.908, 99.968, 99.974, 99.984]
[94.05, 93.93, 93.45, 92.63, 91.53, 89.25, 85.68, 80.84, 76.7, 75.4, 78.1, 82.27, 86.16, 88.57, 90.29, 91.52, 92.19, 92.57, 92.61, 92.66]
